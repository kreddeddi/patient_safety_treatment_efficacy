{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd258ff-1eae-441f-885e-ed223046d75c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS clinicaltrial_silver;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa3add7-d11e-42bc-9a1d-a8c34153d672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Patients table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d81837f-bab5-49f1-80ff-2b8b810f94e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transform and write patients silver table"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the table if it exists to avoid schema merge errors\n",
    "spark.sql(\"DROP TABLE IF EXISTS clinicaltrial_silver.patients\")\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, to_date, upper, when, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_patients_bronze = spark.table(\"clinicaltrial_bronze.patients\")\n",
    "\n",
    "window_spec = Window.partitionBy(\"patient_id\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "\n",
    "df_patients_silver = (\n",
    "    df_patients_bronze\n",
    "    .withColumn(\"enrollment_date\", to_date(\"enrollment_date\"))\n",
    "    .withColumn(\"sex\", upper(col(\"sex\")))\n",
    "    .withColumn(\"sex\", when(col(\"sex\") == \"MALE\", \"M\").when(col(\"sex\") == \"FEMALE\", \"F\").when(col(\"sex\").isNull(), \"U\").otherwise(col(\"sex\")))\n",
    "    .withColumn(\"age\", when(col(\"age\") < 18, lit(18)).when(col(\"age\") > 100, lit(100)).otherwise(col(\"age\")))\n",
    "    .withColumn(\"age\", when(col(\"age\").isNull(), lit(18)).otherwise(col(\"age\")))\n",
    "    .withColumn(\"age\", col(\"age\").cast(IntegerType()))\n",
    "    .withColumn(\"ethnicity\", when(col(\"ethnicity\").isNull(), lit(\"Unknown\")).otherwise(col(\"ethnicity\")))\n",
    "    .withColumn(\"status\", upper(col(\"status\")))\n",
    "    .filter((col(\"age\") >= 18) & (col(\"age\") <= 100))\n",
    "    .withColumn(\"rn\", row_number().over(window_spec))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\", \"source_file\", \"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_patients_silver.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"clinicaltrial_silver.patients\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b9e8b9-f390-4950-9f9e-6b1580c7642d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Visits table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0ec372-3987-437b-8f43-3a06de861276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Drop the table if it exists to avoid schema merge errors\n",
    "spark.sql(\"DROP TABLE IF EXISTS clinicaltrial_silver.visits\")\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_visits_silver = (\n",
    "  spark.table(\"clinicaltrial_bronze.visits\")\n",
    "    .withColumn(\"visit_date\", to_date(\"visit_date\"))    \n",
    "    .filter(col(\"visit_type\").isin([\"Baseline\", \"Week_4\", \"Week_8\", \"Week_12\", \"Week_16\"]))\n",
    "    .withColumn(\"rn\", row_number().over(Window.partitionBy(\"patient_id\", \"visit_type\").orderBy(col(\"visit_date\"))))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .dropDuplicates([\"patient_id\", \"visit_type\", \"visit_date\"])\n",
    "    .drop(\"source_file\", \"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "(df_visits_silver.write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .saveAsTable(\"clinicaltrial_silver.visits\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e776b518-a513-4efc-a9b0-f1b870f09244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Labs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1819e0b5-b361-4586-befb-de9d3b0ceb7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Labs table transformation (fixed cast error)"
    }
   },
   "outputs": [],
   "source": [
    "# Drop table if exists to avoid schema merge errors\n",
    "spark.sql(\"DROP TABLE IF EXISTS clinicaltrial_silver.labs\")\n",
    "\n",
    "from pyspark.sql.functions import when, upper, to_date, col, expr\n",
    "\n",
    "df_labs_silver = (\n",
    "  spark.table(\"clinicaltrial_bronze.labs\")\n",
    "    .withColumn(\"visit_date\", to_date(\"visit_date\"))\n",
    "    .withColumn(\"lab_test\", upper(col(\"lab_test\")))\n",
    "    .withColumn(\"lab_value\", expr(\"try_cast(lab_value as double)\"))\n",
    "    .filter(col(\"lab_value\").isNotNull())\n",
    "    .withColumn(\"abnormal_flag\",\n",
    "                when((col(\"lab_value\") < col(\"normal_low\")) | (col(\"lab_value\") > col(\"normal_high\")), 1)\n",
    "                .otherwise(0))\n",
    "    .drop(\"source_file\", \"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "(\n",
    "  df_labs_silver.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"clinicaltrial_silver.labs\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9129137c-626d-485b-80a4-8cdadd5f7a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Adverse Events table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a913dd7e-e7ad-4a52-bfd7-97afc0715399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop table if exists to avoid schema merge errors\n",
    "spark.sql(\"DROP TABLE IF EXISTS clinicaltrial_silver.adverse_events\")\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, initcap\n",
    "\n",
    "df_ae_silver = (\n",
    "    spark.table(\"clinicaltrial_bronze.adverse_events\")\n",
    "    .withColumn(\"ae_start_date\", to_date(\"ae_start_date\"))\n",
    "    .withColumn(\"ae_end_date\", to_date(\"ae_end_date\"))\n",
    "    .filter((col(\"severity\") >=1) & (col(\"severity\") <= 5))\n",
    "    .filter(col(\"ae_end_date\") >= col(\"ae_start_date\"))\n",
    "    .withColumn(\"ae_term\", initcap(col(\"ae_term\")))\n",
    "    .drop(\"source_file\", \"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_ae_silver.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"clinicaltrial_silver.adverse_events\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73600a85-59e4-4aa8-a7af-e89e91061c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Drug Dosing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56162b33-2655-4945-9f1e-097b46317059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop table if exists to avoid schema merge errors\n",
    "spark.sql(\"DROP TABLE IF EXISTS clinicaltrial_silver.drug_dosing\")\n",
    "          \n",
    "from pyspark.sql.functions import when, upper, to_date, col\n",
    "\n",
    "df_dosing_silver = (\n",
    "    spark.table(\"clinicaltrial_bronze.drug_dosing\")\n",
    "    .withColumn(\"dose_date\", to_date(\"dose_date\"))\n",
    "    .withColumn(\"dose_mg\", col(\"dose_mg\").cast(\"double\"))\n",
    "    .filter(col(\"dose_mg\") >=0)\n",
    "    .withColumn(\"dose_status\", upper(col(\"dose_status\")))\n",
    "    .withColumn(\n",
    "        \"exposed_flag\",\n",
    "        when(col(\"dose_status\") == \"TAKEN\", 1).otherwise(0)\n",
    "    )\n",
    "    .drop(\"source_file\", \"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_dosing_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"clinicaltrial_silver.drug_dosing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0bbc658-bd52-4a89-b04f-2ed57d4f5968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Outcomes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd29f66-e6ea-4990-97f7-3cee031e2084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop table if exists to avoid schema merge errors\n",
    "spark.sql(\"DROP TABLE IF EXISTS clinicaltrial_silver.outcomes\")\n",
    "\n",
    "valid_responses = [\"PD\", \"SD\", \"PR\", \"CR\"]\n",
    "\n",
    "df_outcomes_silver = (\n",
    "    spark.table(\"clinicaltrial_bronze.outcomes\")\n",
    "    .withColumn(\"response_date\", to_date(\"response_date\"))\n",
    "    .withColumn(\"progression_date\", to_date(\"progression_date\"))\n",
    "    .withColumn(\"death_date\", to_date(\"death_date\"))\n",
    "    .filter(col(\"best_response\").isin(valid_responses))\n",
    "    .filter(col(\"progression_date\") >= col(\"response_date\"))\n",
    "    .drop(\"source_file\", \"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_outcomes_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"clinicaltrial_silver.outcomes\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6f978e-78cb-4fb1-aea3-8ea333e1b9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3321078949214520,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03 Silver layer design",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
